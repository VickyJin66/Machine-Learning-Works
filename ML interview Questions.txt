Q1. You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)(https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)

Q2. What is ANOVA:
    https://www.linkedin.com/pulse/working-example-analysis-varianceanova-r-prerna-sahay/
	
	
Q3. Is rotation necessary in PCA? If yes, Why? 
    https://google-interview-hacks.blogspot.com/2017/04/is-rotation-necessary-in-pca-if-yes-why.html
	
Q4. You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. 
    Your manager has asked you to      run    PCA. Would you remove correlated variables first?Why?
	https://www.linkedin.com/pulse/questions-machine-learning-statistics-can-you-answer-saraswat/
	https://stat.ethz.ch/pipermail/r-sig-ecology/2013-March/003624.html
	
Q5. What is a random variable?

Q6. What are the conditions for a function to be a probability mass function?

Q7. What are the conditions for a function to be a probability density function ?

Q8. What is conditional probability? 

Q9. State the Chain rule of conditional probabilities?

Q10. What are expectation, variance and covariance?

Q11. What is a Bernoulli distribution? 

Q12. What is a normal distribution?

Q13. What is the central limit theorem?

Q14. Write the formula for Bayes rule?
Q15. What is Kullback-Leibler (KL) divergence?
Q16. Can KL divergence be used as a distance measure?
Q17. What is Bayes’ Theorem? How is it useful in a machine learning context?
Q18. Why is “Naive” Bayes naive?
Q19. What’s a Fourier transform?

Q20. What is the difference between covariance and correlation?
     https://www.linkedin.com/pulse/covariance-vs-correlation-kumar-p/
	 
	 
Q21. Is it possible capture the correlation between continuous and categorical variable? If yes, how?
     try ANOVA test.
	 or convert variables to numerical using WOE
	 
Q22. What is the Box-Cox transformation used for?
     
Q23. What does P-value signify about the statistical data?

Q24. What do you understand by Hypothesis in the content of Machine Learning?

Q25. How will you find the correlation between a categorical variable and a continuous variable ?

Q26. How to sample from a Normal Distribution with known mean and variance?
     https://docs.scipy.org/doc/numpy-1.14.2/reference/generated/numpy.random.normal.html
	 
KNN Questions:
	 
Q27. How to test and know whether or not we have overfitting problem?

Q28. How is kNN different from k-means clustering?

Q29. Can you explain the difference between a Test Set and a Validation Set?

Q30. How can you avoid overfitting in KNN?

Q31. Which is more important to you– model accuracy, or model performance?
     Model performance
	 
Q32. Can you cite some examples where a false positive is important than a false negative?
     Depends on situation.

	In case of diseases we treat consider False Positive are worst as giving medications to a cancer patient is not that harmful than not giving a treatment to a cancer patient.

	
Q33. Can you cite some examples where a false negative important than a false positive?
    In spam filter example where spam is positive class then False negatives(actually 1 but predicted as 0) are much worse than false positive. As you may click on Email thinking it is not a spam and end up loosing a huge chuck of money for nothing.
	
Q34. Can you cite some examples where both false positive and false negatives are equally important?

     you can typically get these things when your data in balanced. consider an example where you want to predict if the person has type 1 cancer or type 2 cancer. in this can, both are equally important.
	 
Q35. What is the most frequent metric to assess model accuracy for classification problems?
     ROC in case of binary classification else confusion matrix
	 
Q36. Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of- sample evaluation metric?

     https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy

Q37. Why R-Square never decreases on adding new features to the dataset. What metrics we should use at that time?

Q38. which performance measurement is best for multi class classification ?
     There is no "best performance metric". It really depends on what your task is. If you understand your model and output with accuracy only, then it is a good metric for you. For multi class classification, Multi class log-loss is used in many data science competitions. Although not so easily interpretable as accuracy, it penalizes based on your confidence on your predictions. If the models you are using output probabilities, it may be a better way than accuracy to compare and select different models on your validation data, as it takes into account the probabilities and not only the amount of correct predictions. Other measures includes multi class confusion matrix , precision, recall, and F1-score etc. to multi-class settings.
	 
	 
Q39. After analysing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s           true? Without losing any information, can you still build a better model

     o check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.

     But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.
	 
Q40. What are the basic assumptions to be made for linear regression?
    https://www.statisticssolutions.com/assumptions-of-linear-regression/
	
Q41. What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)?

Q42. When would you use GD over SDG, and vice-versa?
     https://elitedatascience.com/machine-learning-interview-questions-answers
	 
Q43. When will you use classification over regression?


Q44. Why isn't Logistic Regression called Logistic Classification?
     https://stats.stackexchange.com/questions/127042/why-isnt-logistic-regression-called-logistic-classification/127044

Q45. if learning rate is too high or low then when does the overfitting & underfitting occurs?
     Smaller learning rates may cause overfitting and larger learning rate may cause underfitting. smaller step sizes may result in the model to learn a more exact solution hence the overfitting. A large learning rate may overshoot and oscillate around minimum so it may underfit, but not always.
	 
Q46. what is the OLS method in mathmatical form & what is the closed form & Gradient Form of OLS


Q47. Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa

Q48. What is convex hull ?
In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.

Q49. What is a large margin classifier?

Q50. SVM being a large margin classifier, is it influenced by outliers? 
    (Yes, if C is large, otherwise not)
	
Q51. What is the role of C in SVM?

Q52. In SVM, what is the angle between the decision boundary and theta?

Q53. What is a kernel in SVM? Why do we use kernels in SVM?

Q54. What is a similarity function in SVM? Why it is named so?

Q55. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?

Q56. How does the SVM parameter C affect the bias/variance trade off? (Remember C = 1/lambda; lambda increases means variance decreases)


Q57. How does the SVM kernel parameter sigma^2 affect the bias/variance trade off?

Q58. Can any similarity function be used for SVM? (No, have to satisfy Mercer’s theorem)

Q59. Logistic regression vs. SVMs: When to use which one?

Q60. What is the difference between supervised and unsupervised machine learning?

Q61. You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?

    Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non – linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions.


Q62. Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which   variable to split at the root node and succeeding nodes?	

Q63. What is entropy?
Q64. What is information gain?
Q65. How are entropy and information gain related vis-a-vis decision trees?
Q66. How do you calculate the entropy of children nodes after the split based on on a feature?
Q67. How do you decide a feature suitability when working with decision tree?
Q68. Explain feature selection using information gain/entropy technique?
Q69. Which algorithm (packaged) is used for building models based on the decision tree?
Q70. What are some of the techniques to decide decision tree pruning?

Q71. Explain about Bag of Words?
Q72. Explain about Text Preprocessing: Stemming, Stop-word removal, Tokenization, Lemmatization.
Q73. Explain about uni-gram, bi-gram, n-grams.?
Q74. What is tf-idf ?
Q75. Why use log in IDF?
Q76. Explain about Word2Vec.?
Q77. Explain about Avg-Word2Vec, tf-idf weighted Word2Vec?
Q78. Explain about Multi-Layered Perceptron (MLP)?
Q79. How to train a single-neuron model?
Q80. How to Train an MLP using Chain rule ?
Q81. How to Train an MLP using Memoization?
Q82. Explain about Backpropagation algorithm?'
Q83. Describe about Vanishing and Exploding Gradient problem?
Q84. Explain about Bias-Variance tradeoff in neural Networks?


Q85. Why is it difficult to train a RNN with SGD?
     RNN is a function of time so time steps are increasing means, our function is getting complex. so it will be in non-convex nature most of the times, that means we may get many local minima's and saddle regions so  Training with plane SGD may be difficult because it may be stuck at local minima or saddle points. 

     
    
Q86. How do you tackle the problem of exploding gradients?
     By gradient clipping
	 
Q87. What is the problem of vanishing gradients? 
     RNN doesn't tend to remember much things from the past
	 https://www.quora.com/What-is-the-vanishing-gradient-problem
	 
Q88. How do you tackle the problem of vanishing gradients?
     https://www.quora.com/What-is-the-vanishing-gradient-problem
	 
Q89. Explain the memory cell of a LSTM. 
     http://colah.github.io/posts/2015-08-Understanding-LSTMs/
	 
	 
Q90. What type of regularization do one use in LSTM?

Q91. How to automatically caption an image? (CNN + LSTM)

Q92. What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?

Q93. How Deep Learning tackles the curse of dimensionality?

Q94. Why do RNNs have a tendency to suffer from exploding/vanishing gradient? How to prevent this? 

Q95. What is the problem with sigmoid during backpropagation?

Q96. What is transfer learning?

Q97. Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN? 

Q98. What is backpropagation through time? (BPTT)

Q99. What is Teacher forcing? Compare and contrast with BPTT.

Q100. What is the disadvantage of using a strict teacher forcing technique? How to solve this?

Q101. What is the difference between LSTM and GRU?

Q102. Explain Gradient Clipping.

Q103. Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?

Q104. You are assigned a new project which involves helping a food delivery company save more money. The problem is, company’s delivery team aren’t able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?

      You might have started hopping through the list of ML algorithms in your mind. But, wait! Such questions are asked to test your machine learning fundamentals.

This is not a machine learning problem. This is a route optimization problem. A machine learning problem consist of three things:

There exist a pattern.
You cannot solve it mathematically (even by writing exponential equations).
You have data on it.
Always look for these three factors to decide if machine learning is a tool to solve a particular problem.


Q105. You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?
      https://www.linkedin.com/pulse/questions-machine-learning-statistics-can-you-answer-saraswat/

Q106. After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?

    As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior result when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.

For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions.


Q107. How is kNN different from kmeans clustering?


Q108. You have built a multiple regression model. Your model R² isn’t as good as you wanted. For improvement, your remove the intercept term, your model R² becomes 0.8 from 0.3. Is it possible? How?

     Yes, it is possible. We need to understand the significance of intercept term in a regression model. The intercept term shows model prediction without any independent variable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is predicted value.  

When intercept term is present, R² value evaluates your model wrt. to the mean model. In absence of intercept term (ymean), the model can make no such evaluation, with large denominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in higher R².

Q109. After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?

      To check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.

But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.

Q110. When is Ridge regression favorable over Lasso regression?

You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.

Conceptually, we can say, lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective.

Q111. While working on a data set, how do you select important variables? Explain your methods.

     Following are the methods of variable selection you can use:

Remove the correlated variables prior to selecting important variables
Use linear regression and select variables based on p values
Use Forward Selection, Backward Selection, Stepwise Selection
Use Random Forest, Xgboost and plot variable importance chart
Use Lasso Regression
Measure information gain for the available set of features and select top n features 


Q112. Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?


The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.

In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.

Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model.

Q113. Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?

Q114. You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?

     The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on unseen sample, it couldn’t find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation.
	 
Q115. You’ve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?

In such high dimensional data sets, we can’t use classical regression techniques, since their assumptions tend to fail. When p > n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.

To combat this situation, we can use penalized regression methods like lasso, LARS, ridge which can shrink the coefficients to reduce variance. Precisely, ridge regression works best in situations where the least square estimates have higher variance.

Among other methods include subset regression, forward stepwise regression.

Q116. We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesn’t. How ?

Don’t get baffled at this question. It’s a simple question asking the difference between the two.

Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: let’s say we have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot encoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.

In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorly used for binary variables.

Q117. What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?

Q118. You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?

We can deal with them in the following ways:

Assign a unique category to missing values, who knows the missing values might decipher some trend
We can remove them blatantly.
Or, we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others.

Q119. ‘People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?

The basic idea for this kind of recommendation engine comes from collaborative filtering.

Collaborative Filtering algorithm considers “User Behavior” for recommending items. They exploit behavior of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known.

Q120. What do you understand by Type I vs Type II error ?

   Type I error is committed when the null hypothesis is true and we reject it, also known as a ‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it, also known as ‘False Negative’.

In the context of confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1).

Q121. In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?
We don’t use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metric can be used in any space to calculate distance. Since, the data points can be present in any dimension, euclidean distance is a more viable option.

Example: Think of a chess board, the movement made by a bishop or a rook is calculated by manhattan distance because of their respective vertical & horizontal movements.

Q122. Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?

Q123. When does regularization becomes necessary in Machine Learning?

Q124. What do you understand by Bias Variance trade off?

Q125. How would you handle an imbalanced dataset?
An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump:

1- Collect more data to even the imbalances in the dataset.

2- Resample the dataset to correct for imbalances.

3- Try a different algorithm altogether on your dataset.

What’s important here is that you have a keen sense for what damage an unbalanced dataset can cause, and how to balance that.

Q126. What’s the F1 score? How would you use it?

The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives don’t matter much.
   
   
Q127. What’s a Fourier transform?

Q128. Explain the difference between L1 and L2 regularization.

Q129.  Explain how a ROC curve works.

Q130. What’s the trade-off between bias and variance?

Q131. What is an auto-encoder?

Q132. What is a Boltzmann Machine?

Q133. What is the role of the activation function?

Q134. Explain Dropout and Batch Normalization. Why BN helps in faster convergence?

Q135. Explain batch gradient descent, stochastic gradient descent and mini-match gradient descent.

Q136. How to find the best hyper parameters?

Q137. Can they derive the back-propagation and weights update?

Q138. Extend the above question to non-trivial layers such as convolutional layers, pooling layers, etc.

Q139. How to implement dropout?

Q140. How's an LSTM gate different from a vanilla RNN?

Q141. What are the data structures in Python?

Q142. what is n-gram?

Q143. Why use feature selection? If two predictors are highly correlated, what is the effect on the coefficients in the logistic regression? What are the confidence intervals of the coefficients?

https://blog.minitab.com/blog/understanding-statistics/handling-multicollinearity-in-regression-analysis
https://etav.github.io/python/vif_factor_python.html
https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/

Q144. What is the role of a data scientist and what actually is data science?

Responsible to explain the logic and stories behind the data, to deliver better product or service, improve the efficiency of a process evolved the data, using tools in statistics and math, machine learning in general. Required knowledge in statistics and machine learning, computer science and applied math. 

Data Science is the practice of inferring and predicting from data. This requires the tools of statistics (data) and scientific method (science). A data scientist is a blend of a computer scientist and statistician. 

Q145. Suppose you have a categorical dependent variable and a mixture of continuous and categorical independent variables, what type of tools/algorithms/methods you could apply for analysis?

Q146. how kmeans works and how kmeans ++ helps initialization.

Q147. What is cross-validation? How to do it right?
https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/

Q148. You are given two sorted arrays, how would you merge them into one single array?
https://www.thedsinterview.com/questions/152

Q149. Given two tables: Facebook_friend_request(requester_id, sent_to_id, time) and Facebook_request_accepted (acceptor_id, requestor_id, time), find the overall acceptance rate of requests.
https://www.thedsinterview.com/questions/5

Q150. How would you quantify the influence of a Twitter user?
https://www.thedsinterview.com/questions/65

Q151. What is probability of pulling a different  color or shape card from a shuttled deck of 52 cards? 
Pick a card lets say Red Diamond 5.
The next card can be any card other than Same shape and same color (i.e, red diamond). It can still be Red hearts (different shape) or any black (different color and different shape)

P (different color or shape) = 1 - P (Same color and shape) = 1- 12/51 = 39/51

Q152. How do you optimize model parameters during model building?
You can do this by using grid search or random search.

Q153. What's wrong with training and testing a machine learning model on the same data?
A robust machine learning model needs to be able to predict on data unseen, predicting on itself will lead to a false sense of high accuracy, usually well above 90 percent, which is overfit.   This also could mean that the model is predicting on the noise, and not the seeing the signal, which means the model is not able to generalize.  

Q154. Write a Python function that displays the first n Fibonacci numbers.
def F(n):
  F = [0,1]
  if n == 0 :
    F.remove(1)
  elif n == 1:
    return F
  else:
    for x in range(2,n+1):
      F.append(F[x-2]+F[x-1])
  return F

Q155. If you have a customer and you know 1. where they live, 2. their income, 3. their gender, 4. their profession, how would you define a machine learning algorithm that predict whether they will “buy today” or “not buy today”?

Q156. In the language of your choice, write code to reverse a string. What data structure would you use? If you have a continuous stream of integers and only wanted to keep the 10 highest numbers, what data structure would you use to implement this?
Q1) Python, using slicing:
def reverser(s):
 return s[::-1]

Q2) A sortable (and ordered), list-type, mutable collection data structure, so in Python, this means a list (versus dictionary, set & tuple), assuming we want to keep duplicate integers.

Q157. Segment a long string into a set of valid words using a dictionary. Return false if the string cannot be segmented. What is the complexity of your solution?

Q158. In Mexico, if you take the mean and the median age, which one will be higher and why?
https://www.thedsinterview.com/questions/22

Q159. Given an list A of objects and another list B which is identical to A except that one element is removed, how would you find that removed element?
If objects re numeric I would find the sum of both lists and the find the difference. If objects are not numeric, I would first sort them out and compare element by element. It first mismatch shows the removed object.

Q160. How could you use GPS data from a car to determine the quality of a driver?
Time: estimated time of arrival versus actual 
Distance: shortest possible paths vs distance traveled
Speed variance: mph driven by driver vs speed limits

These are three factors that track total cost of delivery and influence customer experience.

Q161. Given data on Facebook members friending/defriending each other on Facebook, find out whether a given pair of members are currently friends.

https://www.thedsinterview.com/questions/16

Q162. What are different Time Series forecasting techniques?
Autoregression (AR)
Moving Average (MA)
Autoregressive Moving Average (ARMA)
Autoregressive Integrated Moving Average (ARIMA)
Seasonal Autoregressive Integrated Moving-Average (SARIMA)
Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)
Vector Autoregression (VAR)
Vector Autoregression Moving-Average (VARMA)
Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)
Simple Exponential Smoothing (SES)
Holt Winter’s Exponential Smoothing (HWES)

Q163. Tell me about naive bayes classifier ?

Q164. How do you find the similar documents related to some query sentence/search ?
      convert each document to vector... find similarity...
	  
Q165. So As per my experience, Tf-Idf fails in document classification/clustering ? How can you improve further ?
      try n-grams
	  try tfidf-w2v/ tfidf-glove
	  use DBSCAN
	  
Q166. How do you handle multi-class classification with unbalanced dataset ?
https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a
  
  
Q167. How can I design a chatbot ? 
      SQuAD  dataset and pretrained model for NLP is BERT(Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.
	  
	  
Q168. How does SVM learns non-linear boundaries ? Explain.

Q169. What is precision and recall ? Which one of this do you think is important in medical diagnosis ?
      flase negative(something is positive and model predicting negative) is dangarous. we can go for Recall.
	  
	  https://datascience.stackexchange.com/questions/30881/when-is-precision-more-important-over-recall
	  
Q170. How will you draw ROC for multi class classification problem ?
https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5

Q171. Tell me any other metric to measure multi-class classification result ?
      multiclass log loss

Q172. What is sensitivity and specificity ?

Q173. What is random about Random Forest ?
      RF = DT + Bagging(randomly selected subsets from dataset) + column sampling.
	  
Q174. How do you perform text classification ?

Q175. How can you make sure to learn a context !! Well its not possible with TF-IDF ?
 (I told him about taking n-grams say n = 1, 2, 3, 4 and concatenating tf-idf of them to make a long count vector ?
 
Q176. How does neural networks learns non-linear shapes when it is made of linear nodes ? What makes it learn non-linear boundaries ?
     you have non-linear activation function(e,g sigmoid, tanh..).https://stats.stackexchange.com/questions/222639/what-makes-neural-networks-a-nonlinear-classification-model

Q177. What are the parameters in training a decision tree ?

Q178. what is CHAID?
      there are three ways we can decide splitting of nodes in DT.
      1. Gini Impurity:
      2. Information Gain:
      3. ChiSquare test:https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
	  In CHAID analysis, if the dependent variable is continuous, the F test is used and if the dependent variable is categorical, the chi-square test is used.
	  
	  As indicated in the name, CHAID uses Person’s Chi-square tests of independence, which test for an association between two categorical variables
	  
	  
Q179. What are the criteria for splitting at a node in decision trees ?

Q180. What is the formula of Gini index criteria?

Q181. What is the formula for Entropy criteria?

Q182. How is it decided that on which features it has to split ?

Q183. How do you calculate information gain mathematically ? Are you sure of formula!!

Q184. What is the advantage with random forest ?(reduce variance)

Q185. Tell me about boosting algorithms ?

Q186. how does gradient boosting works ?

Q187. Do you know about Adaboost algorithm ? How and why does it work ?

Q188. What are the kernels used in SVM ? What is the optimization technique of SVM ?

Q189. How does SVM learns the hyperplane ? Talk more about mathematical details ?
      best ever written: https://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/


Q190. Talking about unsupervised learning ? What are the algorithms ?

Q191. How do you decide K in K-Means clustering algorithm ?

Q192. Tell me at least 3 ways of deciding K in clustering ?
        https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/	
        https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html		
       1. Elbow Method
       2. Average silhouette method
       3. A simple and popular solution consists of inspecting the dendrogram produced using hierarchical clustering to see if it suggests a particular number of clusters.

Q193. Can you tell DB-SCAN algorithm ?

Q194. How does HAC (Hierarchical Agglomerative clustering) work ?

Q195. Explain PCA ? Tell me the mathematical steps to implement PCA ?

Q196. What is disadvantage of using PCA ?
      https://www.quora.com/What-are-the-disadvantages-of-a-PCA


Q197. How do you deploy Machine Learning models ?


Q198. Which model would you use in case of unbalanced dataset: Random Forest or Boosting ? Why ?
      Randomforest ==> classweight = balanced.
      
	  Suppose you have an unbalanced data with 80 percent 1 and rest 20 percent 0. Usually when we fit a model like logistic regression or random forest on such a dataset, there are high chances that the model is biased. These models might predict 1 for every data point and will still be correct 80% of the times.
      Gradient Boosting is a sequential process and thus every time it makes an incorrect prediction, it focuses more on that incorrectly predicted data point. So, if the first iteration gave you an accuracy of 80 %, the second iteration would focus on the remaining 20%.	 

	  
Q199. 6. What's the concept behind word to vectors?

Q200. Why are stop words used?

Q201. How to use co-Occurrence matrix?
      https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
      https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285

Q202. Why are the cons of gradient boosting algorithms over linear models?
      takes longer time to train(ndlogn * M)
	  
	  
Q203. What is LGBM?

Q204. How is LGBM different from other gradient boosting algorithms?
      https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc

Q205. Real life example of where you can use normal distribution.
      https://studiousguy.com/real-life-examples-normal-distribution/
	  

Q206. Properties of normal distribution?

Q207. How do you find most important features in a very large dimensional dataset?
      https://www.researchgate.net/post/What_is_the_best_way_for_feature_selection_for_very_high_dimensional_data
	  
	  
Q208. How do you find outliers in knn?
