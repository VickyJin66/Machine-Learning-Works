{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, lr):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4)\n",
    "        self.biases1    = np.random.rand(1)\n",
    "        self.weights2   = np.random.rand(4,1)    \n",
    "        self.biases2    = np.random.rand(1)\n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        self.lr         = lr\n",
    "        \n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        #return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "        return z * (1.0 - z)\n",
    "        \n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.z2 = np.dot(self.input, self.weights1) + self.biases1\n",
    "        self.z3 = np.dot(self.sigmoid(self.z2), self.weights2) + self.biases2\n",
    "        self.output = self.sigmoid(self.z3)\n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "        d_weights2 = np.dot(self.sigmoid(self.z2).T, (2*(self.y - self.output) * self.sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * self.sigmoid_derivative(self.output), self.weights2.T) * self.sigmoid_derivative(self.sigmoid(self.z2))))\n",
    "        d_biases2 = 2*(self.y - self.output) * self.sigmoid_derivative(self.output)\n",
    "        d_biases1 = np.dot(2*(self.y - self.output) * self.sigmoid_derivative(self.output), self.weights2.T) * self.sigmoid_derivative(self.sigmoid(self.z2))\n",
    "        #print(self.weights2, self.weights1)\n",
    "        \n",
    "        self.weights1 += self.lr * d_weights1\n",
    "        self.weights2 += self.lr * d_weights2\n",
    "        \n",
    "        #print(\"after\") \n",
    "        #print(self.weights2, self.weights1)\n",
    "        #print(d_biases2.shape, d_biases1.shape)\n",
    "        self.biases1 = self.biases1 + self.lr * d_biases1\n",
    "        self.biases2 = self.biases2 + self.lr * d_biases2\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1696738 ],\n",
       "       [0.39847618],\n",
       "       [0.69828306],\n",
       "       [0.9797259 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.rand(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07002214]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "def sigmoidPrime(self,z):\n",
    "    #Gradient of sigmoid\n",
    "    return np.exp(-z)/((1+np.exp(-z))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02423679]\n",
      " [0.9769097 ]\n",
      " [0.9780617 ]\n",
      " [0.02415337]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[0,0,1],\n",
    "                  [0,1,1],\n",
    "                  [1,0,1],\n",
    "                  [1,1,1]])\n",
    "    y = np.array([[0],[1],[1],[0]])\n",
    "    nn = NeuralNetwork(X,y)\n",
    "\n",
    "    for i in range(1500):\n",
    "        nn.feedforward()\n",
    "        nn.backprop()\n",
    "\n",
    "    print(nn.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Output After Training:\n",
      "[[0.00966449]\n",
      " [0.99211957]\n",
      " [0.99358898]\n",
      " [0.00786506]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function to normalize inputs\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# sigmoid derivatives to adjust synaptic weights\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# input dataset\n",
    "training_inputs = np.array([[0,0,1],\n",
    "                            [1,1,1],\n",
    "                            [1,0,1],\n",
    "                            [0,1,1]])\n",
    "\n",
    "# output dataset\n",
    "training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0 to create weight matrix, synaptic weights\n",
    "synaptic_weights = 2 * np.random.random((3,1)) - 1\n",
    "\n",
    "print('Random starting synaptic weights: ')\n",
    "print(synaptic_weights)\n",
    "\n",
    "# Iterate 10,000 times\n",
    "for iteration in range(10000):\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = training_inputs\n",
    "    # Normalize the product of the input layer with the synaptic weights\n",
    "    outputs = sigmoid(np.dot(input_layer, synaptic_weights))\n",
    "\n",
    "    # how much did we miss?\n",
    "    error = training_outputs - outputs\n",
    "\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in outputs\n",
    "    adjustments = error * sigmoid_derivative(outputs)\n",
    "\n",
    "    # update weights\n",
    "    synaptic_weights += np.dot(input_layer.T, adjustments)\n",
    "\n",
    "print('Synaptic weights after training: ')\n",
    "print(synaptic_weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01782286]\n",
      " [0.98182786]\n",
      " [0.98183764]\n",
      " [0.01816635]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0,1],\n",
    "                  [0,1,1],\n",
    "                  [1,0,1],\n",
    "                  [1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "nn = NeuralNetwork(X,y,.01)\n",
    "\n",
    "for i in range(80000):\n",
    "    nn.feedforward()\n",
    "    nn.backward()\n",
    "\n",
    "print(nn.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
